{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in the course you have implemented the agent part of each of the experiments. In this notebook you will learn to implement environments in the RL-Glue framework. This is an important part of doing reinforcement learning research. While there are many great existing environments that you can utilize you often will want to create your own to test a specific part of an RL algorithm, or to see how an algorithm handles a specific situation. Being able to create your own environments makes this easy and as you will see in this notebook, creating environments in RL-Glue is really clear and simple.\n",
    "\n",
    "In this notebook we will walk through each of the methods in the Environment class using the example of creating a Mountain Car environment similar to what you saw back in Course 3 Module 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the task in Mountain Car is for an under powered car to make it to the top of a hill:\n",
    "![Mountain Car](mountaincar.png \"Mountain Car\")\n",
    "The car is under-powered so the agent needs to learn to rock back and forth to get enough momentum to reach the goal. At each time step the agent receives from the environment its current velocity (a float between -0.07 and 0.07), and it's current position (a float between -1.2 and 0.5). Because our state is continuous there are a potentially infinite number of states that our agent could be in.\n",
    "\n",
    "\n",
    "Let's walk through what each of the methods of an RL-Glue environment. Then you will implement your own version of lunar lander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```__init__```\n",
    "The init function in your environment is where you you will set up the the variables that you environment will need. In the python implementation of RL-Glue most of the initialization is done in the ```env_init``` method so in the ```__init__``` method we will typically create our variables and initialize them to None. Here is an example of what the Mountain Car environment ```__init__``` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.current_state = None\n",
    "    self.reward_obs_term = None\n",
    "    self.max_velocity = None\n",
    "    self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env_init```\n",
    "The env_init method is where we initialize the environment. For instance we may want to pass in a specific seed for our random function, we may have an MDP that we want to try different lengths of, or any other variables we may wish to initialize our environment with.\n",
    "\n",
    "The ```env_init``` method takes in a dictionary of values we want to pass to the environment. For instance below we could pass in \"max_velocity\" to change the max_velocity the car could reach. We used python's \"get\" method for dictionaries which allows us to set a default value if that key is not found in the dictionary. Below the agent's max_velocity will be 0.07 if no value is passed in for that key.\n",
    "\n",
    "The ```env_init``` method does not return any value and is called when the agent is first initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_init(self, env_info):\n",
    "    \"\"\"Setup for the environment called when the experiment first starts.\n",
    "    Note:\n",
    "        Initialize a tuple with the reward, first state observation, boolean\n",
    "        indicating if it's terminal.\n",
    "    \"\"\"\n",
    "    local_observation = 0\n",
    "    self.max_velocity = env_info.get(\"max_velocity\", 0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env_start```\n",
    "\n",
    "The ```env_start``` method is the first method called by RL-Glue when our experiment starts. Here we want to do things like set what state we want the agent to be in initially (this could be a specific start state, or a random state).\n",
    "\n",
    "The ```env_start``` method returns a observation that RL-Glue then passes to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_start(self):\n",
    "    \"\"\"The first method called when the experiment starts, called before the\n",
    "    agent starts.\n",
    "    Returns:\n",
    "        The first state observation from the environment.\n",
    "    \"\"\"\n",
    "    position = np.random.uniform(-0.6, -0.4)\n",
    "    velocity = 0.0\n",
    "    self.current_state = np.array([position, velocity]) # position, velocity\n",
    "    \n",
    "    return self.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env_step```\n",
    "```env_step``` is taken every time the environment moves forward a step. This is often the most complicated part of the environment. This method takes in the action that the agent has chosen and returns the reward, state, and whether or not the episode terminates.\n",
    "\n",
    "Notice here that the reward is -1.0 on each time step, unless the agent reaches the goal in which case the reward is 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(self, action):\n",
    "    \"\"\"A step taken by the environment.\n",
    "    Args:\n",
    "        action: The action taken by the agent\n",
    "    Returns:\n",
    "        (float, state, Boolean): a tuple of the reward, state observation,\n",
    "            and boolean indicating if it's terminal.\n",
    "    \"\"\"\n",
    "    position, velocity = self.current_state\n",
    "\n",
    "    terminal = False\n",
    "    reward = -1.0\n",
    "    velocity = self.bound_velocity(velocity + 0.001 * (action - 1) - 0.0025 * np.cos(3 * position))\n",
    "    position = self.bound_position(position + velocity)\n",
    "\n",
    "    if position == -1.2:\n",
    "        velocity = 0.0\n",
    "    elif position == 0.5:\n",
    "        self.current_state = None\n",
    "        terminal = True\n",
    "        reward = 0.0\n",
    "\n",
    "    self.current_state = np.array([position, velocity])\n",
    "\n",
    "    self.reward_obs_term = (reward, self.current_state, terminal)\n",
    "\n",
    "    return self.reward_obs_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env_cleanup``` and ```env_message```\n",
    "```env_cleanup``` is run when the environment ends - typically at the end of an episode. ```env_message``` can be used to pass messages into the agent.\n",
    "\n",
    "Often these methods are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_cleanup(self):\n",
    "    \"\"\"Cleanup done after the environment ends\"\"\"\n",
    "    pass\n",
    "\n",
    "def env_message(self, message):\n",
    "    \"\"\"A message asking the environment for information\n",
    "    Args:\n",
    "        message (string): the message passed to the environment\n",
    "    Returns:\n",
    "        string: the response (or answer) to the message\n",
    "    \"\"\"\n",
    "    if message == \"what is the current reward?\":\n",
    "        return \"{}\".format(self.reward_obs_term[0])\n",
    "\n",
    "    # else\n",
    "    return \"I don't know how to respond to your message\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At times agents will have additional functions in the agent class. For instance in Mountain Car ```bound_velocity``` and ```bound_position``` are additional methods that keep the position and velocity within a certain range. Here we use ```self.max_velocity``` set previously, rather than just setting the bounds to (-0.07 and 0.07)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_velocity(self, velocity):\n",
    "    \"\"\"Bounds the velocity of the cart between (-self.max_velocity, self.max_velocity)\"\"\"\n",
    "    if velocity > self.max_velocity:\n",
    "        return self.max_velocity\n",
    "    if velocity < -self.max_velocity:\n",
    "        return -self.max_velocity\n",
    "    return velocity\n",
    "\n",
    "def bound_position(self, position):\n",
    "    \"\"\"Bounds the position of the agent between (-1.2, 0.5)\"\"\"\n",
    "    if position > 0.5:\n",
    "        return 0.5\n",
    "    if position < -1.2:\n",
    "        return -1.2\n",
    "    return position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now implement environments of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
